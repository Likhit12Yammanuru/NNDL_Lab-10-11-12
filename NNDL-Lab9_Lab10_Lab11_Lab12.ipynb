{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "964670fb-84b3-4d69-a071-e489e969cc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with L1 regularization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Training with L2 regularization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Training with Dropout regularization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002941ECB4680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Training with L1_L2 regularization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002941D60EAC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Results saved to C://Users//Admin//Downloads//DDNL//model1_performance.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, log_loss, matthews_corrcoef, accuracy_score, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"C://Users//Admin//Downloads//DDNL//Big-5.csv\"\n",
    "dataset = pd.read_csv(file_path, delimiter=\",\", low_memory=False)\n",
    "\n",
    "# Sample data setup (replace with real dataset columns)\n",
    "X = np.random.rand(1000, 10)\n",
    "y = np.random.randint(0, 3, 1000)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "num_classes = y_train_cat.shape[1]\n",
    "\n",
    "# Data augmentation (simulated by noise addition for tabular data)\n",
    "def augment_data(X, y, factor=2):\n",
    "    X_aug, y_aug = [X], [y]\n",
    "    for _ in range(factor - 1):\n",
    "        noise = np.random.normal(0, 0.01, X.shape)\n",
    "        X_aug.append(X + noise)\n",
    "        y_aug.append(y)\n",
    "    return np.vstack(X_aug), np.vstack(y_aug)\n",
    "\n",
    "X_train_aug, y_train_cat_aug = augment_data(X_train, y_train_cat)\n",
    "\n",
    "# Regularization configurations\n",
    "results = []\n",
    "regularizations = {\n",
    "    \"L1\": l1(0.01),\n",
    "    \"L2\": l2(0.01),\n",
    "    \"Dropout\": None,\n",
    "    \"L1_L2\": l1_l2(l1=0.005, l2=0.005)\n",
    "}\n",
    "\n",
    "for name, reg in regularizations.items():\n",
    "    print(f\"Training with {name} regularization...\")\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=reg))\n",
    "    if name == \"Dropout\":\n",
    "        model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    model.fit(X_train_aug, y_train_cat_aug, epochs=30, batch_size=32, validation_split=0.2, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    auc_roc = roc_auc_score(y_test_cat, y_pred_probs, multi_class='ovr')\n",
    "    logloss = log_loss(y_test_cat, y_pred_probs)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    results.append([name, auc_roc, logloss, mcc, accuracy, precision, recall, f1])\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame(results, columns=[\"Regularization\", \"AUC-ROC\", \"Log Loss\", \"MCC\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
    "results_df.to_csv(\"C://Users//Admin//Downloads//DDNL//model1_performance.csv\", index=False)\n",
    "print(\"Results saved to C://Users//Admin//Downloads//DDNL//model1_performance.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f4ab6b1-1c6a-4464-8646-8606c26d324b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.3216 - loss: 1.2053 - val_accuracy: 0.3812 - val_loss: 1.0969\n",
      "Epoch 2/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3275 - loss: 1.1153 - val_accuracy: 0.3812 - val_loss: 1.0895\n",
      "Epoch 3/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3892 - loss: 1.0913 - val_accuracy: 0.3562 - val_loss: 1.0940\n",
      "Epoch 4/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4185 - loss: 1.0783 - val_accuracy: 0.3125 - val_loss: 1.1015\n",
      "Epoch 5/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4184 - loss: 1.0714 - val_accuracy: 0.2937 - val_loss: 1.1032\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "RNN model performance saved to C://Users//Admin//Downloads//DDNL//rnn_model_performance.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, log_loss, matthews_corrcoef\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"C://Users//Admin//Downloads//DDNL//Big-5.csv\"\n",
    "dataset = pd.read_csv(file_path)\n",
    "\n",
    "# Sample data setup (replace with real dataset columns)\n",
    "X = np.random.rand(1000, 10)  # 1000 samples, 10 features\n",
    "y = np.random.randint(0, 3, 1000)  # 3 classes\n",
    "\n",
    "# Preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape input for RNN [samples, timesteps, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# One-hot encode targets\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "num_classes = y_train_cat.shape[1]\n",
    "\n",
    "# Define RNN model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(64, activation='tanh', input_shape=(1, X_train.shape[2])))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model.fit(X_train, y_train_cat, epochs=30, batch_size=32, validation_split=0.2, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "# Evaluation\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "auc_roc = roc_auc_score(y_test_cat, y_pred_probs, multi_class='ovr')\n",
    "logloss = log_loss(y_test_cat, y_pred_probs)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Results\n",
    "results = pd.DataFrame([['RNN', auc_roc, logloss, mcc, accuracy, precision, recall, f1]],\n",
    "                       columns=[\"Model\", \"AUC-ROC\", \"Log Loss\", \"MCC\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
    "results.to_csv(\"C://Users//Admin//Downloads//DDNL//rnn_model_performance.csv\", index=False)\n",
    "print(\"RNN model performance saved to C://Users//Admin//Downloads//DDNL//rnn_model_performance.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2520367f-5b26-43cb-b56d-756bf93ddd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.3255 - loss: 1.1558 - val_accuracy: 0.3000 - val_loss: 1.1229\n",
      "Epoch 2/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.3788 - loss: 1.1018 - val_accuracy: 0.3250 - val_loss: 1.1332\n",
      "Epoch 3/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3981 - loss: 1.0834 - val_accuracy: 0.3250 - val_loss: 1.1361\n",
      "Epoch 4/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3858 - loss: 1.0828 - val_accuracy: 0.2937 - val_loss: 1.1382\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step\n",
      "Bidirectional RNN model performance saved to C://Users//Admin//Downloads//DDNL//bidir_rnn_model_performance.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, log_loss, matthews_corrcoef\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Bidirectional\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"C://Users//Admin//Downloads//DDNL//Big-5.csv\"\n",
    "dataset = pd.read_csv(file_path)\n",
    "\n",
    "# Sample data setup (replace with real dataset columns)\n",
    "X = np.random.rand(1000, 10)  # 1000 samples, 10 features\n",
    "y = np.random.randint(0, 3, 1000)  # 3 classes\n",
    "\n",
    "# Preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape input for RNN [samples, timesteps, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# One-hot encode targets\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "num_classes = y_train_cat.shape[1]\n",
    "\n",
    "# Define Bidirectional RNN model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(SimpleRNN(64, activation='tanh'), input_shape=(1, X_train.shape[2])))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model.fit(X_train, y_train_cat, epochs=30, batch_size=32, validation_split=0.2, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "# Evaluation\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "auc_roc = roc_auc_score(y_test_cat, y_pred_probs, multi_class='ovr')\n",
    "logloss = log_loss(y_test_cat, y_pred_probs)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Results\n",
    "results = pd.DataFrame([['Bidirectional RNN', auc_roc, logloss, mcc, accuracy, precision, recall, f1]],\n",
    "                       columns=[\"Model\", \"AUC-ROC\", \"Log Loss\", \"MCC\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
    "results.to_csv(\"C://Users//Admin//Downloads//DDNL//bidir_rnn_model_performance.csv\", index=False)\n",
    "print(\"Bidirectional RNN model performance saved to C://Users//Admin//Downloads//DDNL//bidir_rnn_model_performance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4052b88c-1533-41e7-b6fa-1899f070f48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.3388 - loss: 1.0986 - val_accuracy: 0.3750 - val_loss: 1.1024\n",
      "Epoch 2/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3514 - loss: 1.0937 - val_accuracy: 0.3688 - val_loss: 1.1053\n",
      "Epoch 3/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3756 - loss: 1.0911 - val_accuracy: 0.3438 - val_loss: 1.1075\n",
      "Epoch 4/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3939 - loss: 1.0874 - val_accuracy: 0.3500 - val_loss: 1.1102\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000021DAD22BE20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "LSTM model performance saved to C://Users//Admin//Downloads//DDNL//lstm_model_performance.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, log_loss, matthews_corrcoef\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"C://Users//Admin//Downloads//DDNL//Big-5.csv\"\n",
    "dataset = pd.read_csv(file_path)\n",
    "\n",
    "# Sample data setup (replace with real dataset columns)\n",
    "X = np.random.rand(1000, 10)  # 1000 samples, 10 features\n",
    "y = np.random.randint(0, 3, 1000)  # 3 classes\n",
    "\n",
    "# Preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape input for LSTM [samples, timesteps, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# One-hot encode targets\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "num_classes = y_train_cat.shape[1]\n",
    "\n",
    "# Define LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, activation='tanh', input_shape=(1, X_train.shape[2])))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model.fit(X_train, y_train_cat, epochs=30, batch_size=32, validation_split=0.2, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "# Evaluation\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "auc_roc = roc_auc_score(y_test_cat, y_pred_probs, multi_class='ovr')\n",
    "logloss = log_loss(y_test_cat, y_pred_probs)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Results\n",
    "results = pd.DataFrame([['LSTM', auc_roc, logloss, mcc, accuracy, precision, recall, f1]],\n",
    "                       columns=[\"Model\", \"AUC-ROC\", \"Log Loss\", \"MCC\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
    "results.to_csv(\"C://Users//Admin//Downloads//DDNL//lstm_model_performance.csv\", index=False)\n",
    "print(\"LSTM model performance saved to C://Users//Admin//Downloads//DDNL//lstm_model_performance.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3673f58-7288-493e-a5e8-8f419d290178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
